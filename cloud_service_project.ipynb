{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6VyuWF6sV7sRWxRMfifk5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsZZbm4hvsa7","executionInfo":{"status":"ok","timestamp":1767442105264,"user_tz":-120,"elapsed":5193,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"33c6c8ba-8fab-4367-e26f-65f61f48a477"},"outputs":[{"output_type":"stream","name":"stdout","text":["PySpark installed successfully!\n"]}],"source":["!pip install -q pyspark\n","print(\"PySpark installed successfully!\")"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.clustering import KMeans\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.fpm import FPGrowth\n","from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n","import time\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","print(\"All libraries imported!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3W4Rcvobwcf4","executionInfo":{"status":"ok","timestamp":1767442114322,"user_tz":-120,"elapsed":2670,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"ff22be40-b337-4298-e48d-39ad0c4b4771"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["All libraries imported!\n"]}]},{"cell_type":"code","source":["spark = SparkSession.builder \\\n","    .appName(\"CloudDataProcessing\") \\\n","    .config(\"spark.driver.memory\", \"4g\") \\\n","    .getOrCreate()\n","\n","spark.sparkContext.setLogLevel(\"ERROR\")\n","\n","print(\"Spark session created!\")\n","print(f\"Spark version: {spark.version}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BzsuJUwwnOy","executionInfo":{"status":"ok","timestamp":1767442156345,"user_tz":-120,"elapsed":15589,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"7c220625-1fb8-4344-b7f9-f1c415196342"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Spark session created!\n","Spark version: 4.0.1\n"]}]},{"cell_type":"code","source":["!mkdir -p /content/results\n","print(\"Results directory created at /content/results/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwDYZwUMwun9","executionInfo":{"status":"ok","timestamp":1767442170962,"user_tz":-120,"elapsed":119,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"9954e02a-332c-4fe6-f166-e15cb4a7d51e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Results directory created at /content/results/\n"]}]},{"cell_type":"code","source":["filename = \"large_data.csv\"\n","\n","df = spark.read.csv(filename, header=True, inferSchema=True)\n","\n","print(f\"Data loaded successfully!\")\n","print(f\"Number of rows: {df.count():,}\")\n","print(f\"Number of columns: {len(df.columns)}\")\n","print(\"\\nColumn names:\", df.columns)\n","print(\"\\nFirst 5 rows:\")\n","df.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12rLFGbGw44k","executionInfo":{"status":"ok","timestamp":1767443149487,"user_tz":-120,"elapsed":3134,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"dacbc45a-ebbf-4a50-bda6-276aae5e3ac4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded successfully!\n","Number of rows: 500,000\n","Number of columns: 7\n","\n","Column names: ['order_id', 'customer_id', 'product_id', 'quantity', 'price', 'region', 'total_price']\n","\n","First 5 rows:\n","+--------+-----------+----------+--------+------+---------+-----------+\n","|order_id|customer_id|product_id|quantity| price|   region|total_price|\n","+--------+-----------+----------+--------+------+---------+-----------+\n","|       1|      25795|       308|       1|799.12|Northeast|     799.12|\n","|       2|      10860|       695|       9|501.99|  Midwest|    4517.91|\n","|       3|      48158|       905|      12|577.14|Southeast|    6925.68|\n","|       4|      21284|       215|      12|156.09|    North|    1873.08|\n","|       5|      16265|       229|      17|798.67|     East|   13577.39|\n","+--------+-----------+----------+--------+------+---------+-----------+\n","only showing top 5 rows\n"]}]},{"cell_type":"code","source":["num_rows = df.count()\n","num_cols = len(df.columns)\n","\n","print(f\"Total Rows: {num_rows:,}\")\n","print(f\"Total Columns: {num_cols}\")\n","print(\"\\nColumn Information:\")\n","for col_name, col_type in df.dtypes:\n","    print(f\"  {col_name}: {col_type}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2xchyJX0lWR","executionInfo":{"status":"ok","timestamp":1767443188593,"user_tz":-120,"elapsed":529,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"8cef7291-eb64-4b6e-b5bf-c096dfa31e94"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Rows: 500,000\n","Total Columns: 7\n","\n","Column Information:\n","  order_id: int\n","  customer_id: int\n","  product_id: int\n","  quantity: int\n","  price: double\n","  region: string\n","  total_price: double\n"]}]},{"cell_type":"code","source":["num_columns = [field.name for field in df.schema.fields\n","               if isinstance(field.dataType, (IntegerType, LongType, FloatType, DoubleType))]\n","\n","print(f\"Found {len(num_columns)} numerical columns\\n\")\n","\n","for col in num_columns:\n","    stats = df.select(\n","        F.min(col).alias('min'),\n","        F.max(col).alias('max'),\n","        F.mean(col).alias('mean'),\n","        F.stddev(col).alias('std')\n","    ).collect()[0]\n","\n","    print(f\"{col}:\")\n","    print(f\"  Min: {stats['min']}\")\n","    print(f\"  Max: {stats['max']}\")\n","    print(f\"  Mean: {stats['mean']:.2f}\")\n","    print(f\"  Std Dev: {stats['std']:.2f}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJGI4E-10wee","executionInfo":{"status":"ok","timestamp":1767443235845,"user_tz":-120,"elapsed":8851,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"02390d02-f0a6-464d-bf1e-449a59f71ba4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6 numerical columns\n","\n","order_id:\n","  Min: 1\n","  Max: 500000\n","  Mean: 250000.50\n","  Std Dev: 144337.71\n","\n","customer_id:\n","  Min: 10000\n","  Max: 49999\n","  Mean: 29995.33\n","  Std Dev: 11541.41\n","\n","product_id:\n","  Min: 100\n","  Max: 999\n","  Mean: 549.64\n","  Std Dev: 259.95\n","\n","quantity:\n","  Min: 1\n","  Max: 19\n","  Mean: 10.01\n","  Std Dev: 5.48\n","\n","price:\n","  Min: 10.0\n","  Max: 1000.0\n","  Mean: 505.28\n","  Std Dev: 286.00\n","\n","total_price:\n","  Min: 10.01\n","  Max: 18999.62\n","  Mean: 5056.86\n","  Std Dev: 4282.43\n","\n"]}]},{"cell_type":"code","source":["total_rows = df.count()\n","\n","for col in df.columns:\n","    null_count = df.filter(F.col(col).isNull()).count()\n","    percentage = (null_count / total_rows * 100) if total_rows > 0 else 0\n","    print(f\"{col}: {null_count} missing ({percentage:.1f}%)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkGTWF5p06cy","executionInfo":{"status":"ok","timestamp":1767443271569,"user_tz":-120,"elapsed":8021,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"bd6cf1c0-680b-47f5-cde7-0bff07905998"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["order_id: 0 missing (0.0%)\n","customer_id: 0 missing (0.0%)\n","product_id: 0 missing (0.0%)\n","quantity: 0 missing (0.0%)\n","price: 0 missing (0.0%)\n","region: 0 missing (0.0%)\n","total_price: 0 missing (0.0%)\n"]}]},{"cell_type":"code","source":["for col in df.columns:\n","    unique_count = df.select(col).distinct().count()\n","    print(f\"{col}: {unique_count} unique values\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AeaPNiz91Bv9","executionInfo":{"status":"ok","timestamp":1767443305544,"user_tz":-120,"elapsed":16574,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"2f048726-f359-4676-a19e-fe5b3de52b40"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["order_id: 500000 unique values\n","customer_id: 40000 unique values\n","product_id: 900 unique values\n","quantity: 19 unique values\n","price: 98353 unique values\n","region: 8 unique values\n","total_price: 341714 unique values\n"]}]},{"cell_type":"code","source":["\n","if len(num_columns) >= 2:\n","    print(\"\\nCorrelations between numerical columns:\\n\")\n","    for i in range(len(num_columns)):\n","        for j in range(i+1, len(num_columns)):\n","            col1 = num_columns[i]\n","            col2 = num_columns[j]\n","            corr = df.stat.corr(col1, col2)\n","            print(f\"{col1} <-> {col2}: {corr:.3f}\")\n","else:\n","    print(\"Need at least 2 numerical columns for correlation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eAGDl6rQ1Oag","executionInfo":{"status":"ok","timestamp":1767443349544,"user_tz":-120,"elapsed":19341,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"56095199-c30a-4cf7-fdc6-fe5f05011c9b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Correlations between numerical columns:\n","\n","order_id <-> customer_id: 0.001\n","order_id <-> product_id: 0.001\n","order_id <-> quantity: 0.000\n","order_id <-> price: -0.003\n","order_id <-> total_price: -0.002\n","customer_id <-> product_id: -0.001\n","customer_id <-> quantity: -0.001\n","customer_id <-> price: 0.002\n","customer_id <-> total_price: 0.001\n","product_id <-> quantity: 0.000\n","product_id <-> price: -0.001\n","product_id <-> total_price: -0.001\n","quantity <-> price: 0.000\n","quantity <-> total_price: 0.647\n","price <-> total_price: 0.669\n"]}]},{"cell_type":"code","source":["if len(num_columns) >= 2:\n","\n","    label_col = num_columns[-1]\n","    feature_cols = num_columns[:-1]\n","\n","    print(f\"Target column: {label_col}\")\n","    print(f\"Feature columns: {', '.join(feature_cols)}\\n\")\n","\n","\n","    df_clean = df.select(feature_cols + [label_col]).na.drop()\n","\n","\n","    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","    df_assembled = assembler.transform(df_clean)\n","\n","\n","    train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)\n","\n","    print(f\"Training samples: {train_data.count()}\")\n","    print(f\"Testing samples: {test_data.count()}\\n\")\n","\n","\n","    lr = LinearRegression(featuresCol=\"features\", labelCol=label_col, maxIter=100)\n","\n","    start_time = time.time()\n","    model = lr.fit(train_data)\n","    training_time = time.time() - start_time\n","\n","\n","    predictions = model.transform(test_data)\n","\n","\n","    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\")\n","    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","    mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","\n","    print(\"Results:\")\n","    print(f\"  RMSE (Root Mean Squared Error): {rmse:.4f}\")\n","    print(f\"  R² (R-squared): {r2:.4f}\")\n","    print(f\"  MAE (Mean Absolute Error): {mae:.4f}\")\n","    print(f\"  Training Time: {training_time:.2f} seconds\\n\")\n","\n","\n","    predictions.select(label_col, \"prediction\").limit(100) \\\n","        .coalesce(1).write.mode(\"overwrite\") \\\n","        .csv(\"/content/results/linear_regression_predictions\", header=True)\n","\n","    print(\"Predictions saved to /content/results/linear_regression_predictions/\")\n","\n","else:\n","    print(\"Need at least 2 numerical columns for Linear Regression\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKC1qowL1XnI","executionInfo":{"status":"ok","timestamp":1767443465110,"user_tz":-120,"elapsed":26202,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"9b12346a-2e57-48f2-f5a1-6d282d9f322a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Target column: total_price\n","Feature columns: order_id, customer_id, product_id, quantity, price\n","\n","Training samples: 399895\n","Testing samples: 100105\n","\n","Results:\n","  RMSE (Root Mean Squared Error): 1571.8279\n","  R² (R-squared): 0.8658\n","  MAE (Mean Absolute Error): 1177.5824\n","  Training Time: 7.31 seconds\n","\n","Predictions saved to /content/results/linear_regression_predictions/\n"]}]},{"cell_type":"code","source":["if len(num_columns) >= 2:\n","    selected_cols = num_columns[:min(5, len(num_columns))]\n","    print(f\"Using columns: {', '.join(selected_cols)}\\n\")\n","\n","\n","    df_clean = df.select(selected_cols).na.drop()\n","\n","\n","    assembler = VectorAssembler(inputCols=selected_cols, outputCol=\"features\")\n","    df_assembled = assembler.transform(df_clean)\n","\n","\n","    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n","    scaler_model = scaler.fit(df_assembled)\n","    df_scaled = scaler_model.transform(df_assembled)\n","\n","\n","    k = 5\n","    kmeans = KMeans(featuresCol=\"scaled_features\", k=k, seed=42, maxIter=20)\n","\n","    start_time = time.time()\n","    model = kmeans.fit(df_scaled)\n","    training_time = time.time() - start_time\n","\n","\n","    predictions = model.transform(df_scaled)\n","\n","\n","    cluster_counts = predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").collect()\n","\n","    print(f\"Number of clusters: {k}\")\n","    print(f\"Total samples: {df_clean.count()}\\n\")\n","    print(\"Cluster distribution:\")\n","    for row in cluster_counts:\n","        print(f\"  Cluster {row['prediction']}: {row['count']} samples\")\n","\n","    print(f\"\\nTraining Time: {training_time:.2f} seconds\\n\")\n","\n","\n","    predictions.select(selected_cols + [\"prediction\"]).limit(100) \\\n","        .coalesce(1).write.mode(\"overwrite\") \\\n","        .csv(\"/content/results/kmeans_clusters\", header=True)\n","\n","    print(\"Cluster assignments saved to /content/results/kmeans_clusters/\")\n","\n","else:\n","    print(\"Need at least 2 numerical columns for K-Means\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8uYdkul41pw3","executionInfo":{"status":"ok","timestamp":1767443586144,"user_tz":-120,"elapsed":31841,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"cb2514c3-70ae-470d-d3a0-fa8fb3dc9979"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using columns: order_id, customer_id, product_id, quantity, price\n","\n","Number of clusters: 5\n","Total samples: 500000\n","\n","Cluster distribution:\n","  Cluster 0: 95698 samples\n","  Cluster 1: 103377 samples\n","  Cluster 2: 97164 samples\n","  Cluster 3: 100012 samples\n","  Cluster 4: 103749 samples\n","\n","Training Time: 23.68 seconds\n","\n","Cluster assignments saved to /content/results/kmeans_clusters/\n"]}]},{"cell_type":"code","source":["if len(num_columns) >= 2:\n","    label_col = num_columns[0]\n","    feature_cols = num_columns[1:]\n","\n","    print(f\"Creating binary label from: {label_col}\")\n","    print(f\"Feature columns: {', '.join(feature_cols)}\\n\")\n","\n","\n","    df_clean = df.select(feature_cols + [label_col]).na.drop()\n","\n","\n","    median_value = df_clean.approxQuantile(label_col, [0.5], 0.01)[0]\n","    df_labeled = df_clean.withColumn(\"label\",\n","                                     F.when(F.col(label_col) >= median_value, 1).otherwise(0))\n","\n","\n","    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","    df_assembled = assembler.transform(df_labeled)\n","\n","\n","    train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)\n","\n","    print(f\"Training samples: {train_data.count()}\")\n","    print(f\"Testing samples: {test_data.count()}\\n\")\n","\n","\n","    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n","\n","    start_time = time.time()\n","    model = lr.fit(train_data)\n","    training_time = time.time() - start_time\n","\n","\n","    predictions = model.transform(test_data)\n","\n","\n","    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n","    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n","    f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n","\n","    print(\"Results:\")\n","    print(f\"  Accuracy: {accuracy:.4f}\")\n","    print(f\"  F1-Score: {f1:.4f}\")\n","    print(f\"  Training Time: {training_time:.2f} seconds\\n\")\n","\n","\n","    predictions.select(\"label\", \"prediction\", F.col(\"probability\").cast(\"string\").alias(\"probability\")) \\\n","        .limit(100) \\\n","        .coalesce(1).write.mode(\"overwrite\") \\\n","        .csv(\"/content/results/logistic_regression_predictions\", header=True)\n","\n","    print(\"Predictions saved to /content/results/logistic_regression_predictions/\")\n","\n","else:\n","    print(\"Need at least 2 numerical columns for Logistic Regression\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fk3fAaqz2Rbk","executionInfo":{"status":"ok","timestamp":1767444401780,"user_tz":-120,"elapsed":27402,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"d9fb56ff-a932-4ed5-f15c-e6e4c040aa40"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating binary label from: order_id\n","Feature columns: customer_id, product_id, quantity, price, total_price\n","\n","Training samples: 399895\n","Testing samples: 100105\n","\n","Results:\n","  Accuracy: 0.4998\n","  F1-Score: 0.4998\n","  Training Time: 9.05 seconds\n","\n","Predictions saved to /content/results/logistic_regression_predictions/\n"]}]},{"cell_type":"code","source":["string_cols = [field.name for field in df.schema.fields\n","               if isinstance(field.dataType, StringType)]\n","\n","if len(string_cols) >= 1:\n","    selected_cols = string_cols[:min(3, len(string_cols))]\n","    print(f\"Using columns: {', '.join(selected_cols)}\\n\")\n","\n","\n","    df_transactions = df.select(\n","        F.array(*[F.col(c) for c in selected_cols]).alias(\"items\")\n","    ).filter(F.size(\"items\") > 0)\n","\n","    df_transactions = df_transactions.withColumn(\n","        \"items\",\n","        F.array_except(\"items\", F.array(F.lit(None)))\n","    ).filter(F.size(\"items\") > 0)\n","\n","    print(f\"Total transactions: {df_transactions.count()}\\n\")\n","\n","    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.5)\n","\n","    start_time = time.time()\n","    model = fpGrowth.fit(df_transactions)\n","    training_time = time.time() - start_time\n","\n","    frequent_itemsets = model.freqItemsets\n","    print(\"Top 10 Frequent Itemsets:\")\n","    frequent_itemsets.orderBy(F.desc(\"freq\")).show(10, truncate=False)\n","\n","    association_rules = model.associationRules\n","    print(\"\\nTop 10 Association Rules:\")\n","    association_rules.orderBy(F.desc(\"confidence\")).show(10, truncate=False)\n","\n","    print(f\"\\nTraining Time: {training_time:.2f} seconds\\n\")\n","\n","    frequent_itemsets.orderBy(F.desc(\"freq\")) \\\n","        .select(F.col(\"items\").cast(\"string\").alias(\"items\"), \"freq\") \\\n","        .limit(50) \\\n","        .coalesce(1).write.mode(\"overwrite\") \\\n","        .csv(\"/content/results/frequent_itemsets\", header=True)\n","\n","    association_rules.orderBy(F.desc(\"confidence\")) \\\n","        .select(F.col(\"antecedent\").cast(\"string\"), F.col(\"consequent\").cast(\"string\"), \"confidence\", \"lift\") \\\n","        .limit(50) \\\n","        .coalesce(1).write.mode(\"overwrite\") \\\n","        .csv(\"/content/results/association_rules\", header=True)\n","\n","    print(\"Frequent itemsets saved to /content/results/frequent_itemsets/\")\n","    print(\"Association rules saved to /content/results/association_rules/\")\n","\n","else:\n","    print(\"Need at least 1 string column for FP-Growth\")\n","    print(\"Creating sample transactions for demonstration...\\n\")\n","\n","    sample_data = [\n","        ([\"milk\", \"bread\", \"butter\"],),\n","        ([\"milk\", \"bread\"],),\n","        ([\"milk\", \"butter\"],),\n","        ([\"bread\", \"butter\"],),\n","        ([\"milk\", \"bread\", \"butter\", \"cheese\"],)\n","    ] * 100\n","\n","    df_sample = spark.createDataFrame(sample_data, [\"items\"])\n","\n","    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.3, minConfidence=0.6)\n","    model = fpGrowth.fit(df_sample)\n","\n","    print(\"Sample Frequent Itemsets:\")\n","    model.freqItemsets.show(10, truncate=False)\n","\n","    print(\"\\nSample Association Rules:\")\n","    model.associationRules.show(10, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrX8JZDQ2qQ1","executionInfo":{"status":"ok","timestamp":1767449262740,"user_tz":-120,"elapsed":6771,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"688b73f3-5b2b-4499-934b-552e6fee8d93"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Using columns: region\n","\n","Total transactions: 500000\n","\n","Top 10 Frequent Itemsets:\n","+-----------+-----+\n","|items      |freq |\n","+-----------+-----+\n","|[West]     |62747|\n","|[East]     |62741|\n","|[South]    |62612|\n","|[North]    |62605|\n","|[Southeast]|62442|\n","|[Midwest]  |62368|\n","|[Southwest]|62321|\n","|[Northeast]|62164|\n","+-----------+-----+\n","\n","\n","Top 10 Association Rules:\n","+----------+----------+----------+----+-------+\n","|antecedent|consequent|confidence|lift|support|\n","+----------+----------+----------+----+-------+\n","+----------+----------+----------+----+-------+\n","\n","\n","Training Time: 2.22 seconds\n","\n","Frequent itemsets saved to /content/results/frequent_itemsets/\n","Association rules saved to /content/results/association_rules/\n"]}]},{"cell_type":"code","source":["if len(num_columns) >= 2:\n","    selected_cols = num_columns[:min(5, len(num_columns))]\n","    df_clean = df.select(selected_cols).na.drop()\n","\n","    node_counts = [1, 2, 4, 8]\n","    execution_times = []\n","\n","    for nodes in node_counts:\n","        print(f\"Running with {nodes} node(s)...\")\n","\n","        df_partitioned = df_clean.repartition(nodes)\n","\n","        spark.conf.set(\"spark.default.parallelism\", nodes)\n","        spark.conf.set(\"spark.sql.shuffle.partitions\", nodes * 2)\n","\n","        assembler = VectorAssembler(inputCols=selected_cols, outputCol=\"features\")\n","        df_assembled = assembler.transform(df_partitioned)\n","\n","        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n","        df_scaled = scaler.fit(df_assembled).transform(df_assembled)\n","\n","        kmeans = KMeans(featuresCol=\"scaled_features\", k=3, seed=42, maxIter=10)\n","\n","        start_time = time.time()\n","        model = kmeans.fit(df_scaled)\n","        predictions = model.transform(df_scaled)\n","        predictions.count()\n","        exec_time = time.time() - start_time\n","\n","        execution_times.append(exec_time)\n","        print(f\"  ✓ Completed in {exec_time:.2f} seconds\\n\")\n","\n","    baseline_time = execution_times[0]\n","\n","    results = []\n","    for i, nodes in enumerate(node_counts):\n","        speedup = baseline_time / execution_times[i]\n","        efficiency = (speedup / nodes) * 100\n","\n","        results.append({\n","            'Nodes': nodes,\n","            'Time (s)': round(execution_times[i], 2),\n","            'Speedup': round(speedup, 3),\n","            'Efficiency (%)': round(efficiency, 2)\n","        })\n","\n","    print(\"=\" * 70)\n","    print(\"PERFORMANCE RESULTS\")\n","    print(\"=\" * 70)\n","    print(f\"{'Nodes':<10} {'Time (s)':<15} {'Speedup':<15} {'Efficiency (%)':<15}\")\n","    print(\"-\" * 70)\n","\n","    for r in results:\n","        print(f\"{r['Nodes']:<10} {r['Time (s)']:<15} {r['Speedup']:<15} {r['Efficiency (%)']:<15}\")\n","\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"FORMULAS USED:\")\n","    print(\"  Speedup(n) = T(1) / T(n)\")\n","    print(\"  Efficiency(n) = Speedup(n) / n × 100%\")\n","    print(\"=\" * 70)\n","\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv('/content/results/performance_benchmark.csv', index=False)\n","    print(\"\\n✅ Performance results saved to /content/results/performance_benchmark.csv\")\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8HlhukMeFClC","executionInfo":{"status":"ok","timestamp":1767449611133,"user_tz":-120,"elapsed":81042,"user":{"displayName":"Zyad Astal","userId":"15278585485498984401"}},"outputId":"8cfb4445-bd58-40fa-bfb4-083e7d0cf190"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Running with 1 node(s)...\n","  ✓ Completed in 16.34 seconds\n","\n","Running with 2 node(s)...\n","  ✓ Completed in 15.97 seconds\n","\n","Running with 4 node(s)...\n","  ✓ Completed in 18.43 seconds\n","\n","Running with 8 node(s)...\n","  ✓ Completed in 18.70 seconds\n","\n","======================================================================\n","PERFORMANCE RESULTS\n","======================================================================\n","Nodes      Time (s)        Speedup         Efficiency (%) \n","----------------------------------------------------------------------\n","1          16.34           1.0             100.0          \n","2          15.97           1.024           51.19          \n","4          18.43           0.887           22.17          \n","8          18.7            0.874           10.92          \n","\n","======================================================================\n","FORMULAS USED:\n","  Speedup(n) = T(1) / T(n)\n","  Efficiency(n) = Speedup(n) / n × 100%\n","======================================================================\n","\n","✅ Performance results saved to /content/results/performance_benchmark.csv\n"]}]}]}